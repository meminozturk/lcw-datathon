{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "import scipy\n",
    "from contextlib import contextmanager\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, accuracy_score, confusion_matrix, recall_score, precision_score, f1_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "FEATS_EXCLUDED = [\"Gun\",\"MagazaID\",\"MerchAltGrupID\",\"UrunKlasmanID\",\"SatisAdet\",\"MerchGrup\",\"MerchGrupID\",\"MerchMarkaYasGrupID\",\"BuyerGrupID\",\"KlasmanGrupID\",\"VucutBolge\",\"KlasmanIklimStatu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "    \n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances.png')\n",
    "\n",
    "def thr_to_accuracy(thr, Y_test, predictions):\n",
    "    return -accuracy_score(Y_test, np.array(predictions>thr, dtype=np.int))\n",
    "\n",
    "#from sklearn.utils import check_arrays\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "\n",
    "def mape(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / (y_true+0.00000000001))) * 100\n",
    "\n",
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "def kfold_lightgbm(params, train_df, test_df, num_folds,FEATS_EXCLUDED):\n",
    "    print(\"Starting LightGBM. Train shape: {}\".format(train_df.shape))\n",
    "    folds = KFold(n_splits= num_folds, shuffle=True, random_state=326)\n",
    "\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    final_rmse = 0\n",
    "    final_mape = 0\n",
    "    final_mae = 0\n",
    "    feats = [f for f in train_df.columns if f not in FEATS_EXCLUDED]\n",
    "    clfs = []\n",
    "    # k-fold\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['SatisAdet'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['SatisAdet'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['SatisAdet'].iloc[valid_idx]\n",
    "\n",
    "        # set data structure\n",
    "        lgb_train = lgb.Dataset(train_x,label=train_y,free_raw_data=False)\n",
    "        lgb_test = lgb.Dataset(valid_x,label=valid_y,free_raw_data=False)\n",
    "\n",
    "        reg = lgb.train(params,lgb_train,valid_sets=[lgb_train, lgb_test],valid_names=['train', 'valid'],\n",
    "                        num_boost_round=100000,early_stopping_rounds= 100,verbose_eval=200)\n",
    "\n",
    "        oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)\n",
    "        sub_preds += reg.predict(test_df[feats], num_iteration=reg.best_iteration) / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = np.log1p(reg.feature_importance(importance_type='gain', iteration=reg.best_iteration))\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        curr_rmse = sqrt(mean_squared_error(valid_y, oof_preds[valid_idx]))\n",
    "        curr_mape = mape(valid_y, oof_preds[valid_idx]) \n",
    "        curr_mae = mean_absolute_error(valid_y, oof_preds[valid_idx])\n",
    "        \n",
    "        #curr_rmse = roc_auc_score(valid_y, oof_preds[valid_idx])\n",
    "        final_rmse += curr_rmse/num_folds\n",
    "        final_mape += curr_mape/num_folds\n",
    "        final_mae += curr_mae/num_folds\n",
    "        print('Fold %2d rmse : %.6f' % (n_fold + 1, curr_rmse))\n",
    "        print('Fold %2d mape : %.6f' % (n_fold + 1, curr_mape))\n",
    "        print('Fold %2d mae : %.6f' % (n_fold + 1, curr_mae))\n",
    "\n",
    "        del reg, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "              \n",
    "    print('Averall RMSE : %.6f' % (final_rmse))\n",
    "    print('Averall MAPE : %.6f' % (final_mape))\n",
    "    print('Averall MAE : %.6f' % (final_mae))\n",
    "\n",
    "    # save submission file\n",
    "    #test_df.loc[:,'SatisAdet'] = sub_preds\n",
    "    #test_df = test_df.reset_index()\n",
    "    #test_df = test_df[['card_id', 'SatisAdet']]\n",
    "    #submission_file_name = \"lgbm_\"+str(np.round(final_rmse,4))+\".csv\"\n",
    "    #test_df.to_csv(DATAPATH+submission_file_name, index=False)    \n",
    "        \n",
    "    return sub_preds, oof_preds, feature_importance_df, final_rmse    \n",
    "\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(trn_series=None, \n",
    "                  #tst_series=None, \n",
    "                  target=None, \n",
    "                  min_samples_leaf=1, \n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    assert len(trn_series) == len(target)\n",
    "    #assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean \n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index \n",
    "    \"\"\"\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    \"\"\"\n",
    "    return add_noise(ft_trn_series, noise_level) #, add_noise(ft_tst_series, noise_level)\n",
    "\n",
    "def missing_fun(data):\n",
    "    missing_value_df = data.dtypes.to_frame(\"type\").reset_index()\n",
    "    percent_missing = data.isnull().sum() * 100 / len(data)\n",
    "    nunique = data.nunique(dropna=False).values\n",
    "    missing_value_df[\"percent_missing\"] = percent_missing.values\n",
    "    missing_value_df[\"nunique\"] = nunique\n",
    "    missing_value_df = missing_value_df.loc[missing_value_df.percent_missing !=0]\n",
    "    missing_value_df = missing_value_df.sort_values(by=\"percent_missing\", ascending=False)\n",
    "    return missing_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/SatisiKesfet_TrainData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[\"Markup\"] = 1-train[\"Markup\"]\n",
    "train[\"SezonGrup\"] = train[\"SezonGrup\"].apply(lambda x: x.strip())\n",
    "train[\"SezonGrup\"] = train[\"SezonGrup\"].map({'Y': 1, 'K': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "klasman = pd.read_csv(\"data/Dim_Klasman.csv\")\n",
    "magaza = pd.read_csv(\"data/Dim_Magaza.csv\")\n",
    "ozelguntanimlari = pd.read_csv(\"data/Dim_OzelGunTanimlari.csv\")\n",
    "tarih = pd.read_csv(\"data/Dim_Tarih.csv\")\n",
    "meteoroloji = pd.read_csv(\"data/MeteorolojiDegerleri.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.merge(tarih[[\"Gun\",\"Yil\",\"Ay\",\"YilHafta\"]],\"left\",[\"Gun\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.merge(magaza[[\"MagazaID\",\"SehirID\",\"OutletMi\"]],\"left\",[\"MagazaID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.merge(meteoroloji,\"left\",[\"Gun\",\"SehirID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(\"SehirID\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in [\"MinimumSicaklik\",\"OrtalamaSicaklik\",\"MaksimumSicaklik\",\"YagisMiktari\",\"KarKalinligi\",\"Yagmur\",\"Kar\"]:\n",
    "    train[col] = train.groupby(\"YilHafta\")[col].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FEATS_EXCLUDED = [\"Gun\",\"MagazaID\",\"MerchAltGrupID\",\"UrunKlasmanID\",\"SatisAdet\",\"MerchGrup\",\"MerchGrupID\",\"MerchMarkaYasGrupID\",\"BuyerGrupID\",\"KlasmanGrupID\",\"VucutBolge\",\"KlasmanIklimStatu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[\"GunSonuDepoStok\"] = train[\"GunSonuToplamStok\"] - train[\"GunSonuReyonStok\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = [\"OrtBirimFiyat\",\"OrtBirimMaliyet\",\"IndirimOrani\",\"KarMarji\"]\n",
    "mask = (train.OrtBirimFiyat == 0)&(train.OrtBirimMaliyet == 0)&(train.IndirimOrani == 0)&(train.KarMarji == 0)\n",
    "train.loc[mask, cols] = np.nan\n",
    "train[cols] = train.groupby(['MerchAltGrupID', 'UrunKlasmanID'])[cols].transform(lambda x: x.fillna(x.mean()))\n",
    "train[cols] = train[cols].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sezon BazÄ±nda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "season_sum_cols = ['GunSonuReyonStok', 'GunSonuToplamStok','ModelSayisi','GunSonuDepoStok',\"SatisAdet\"]\n",
    "season_mean_cols = [\"OrtBirimFiyat\",\"OrtBirimMaliyet\",\"IndirimOrani\",\"KarMarji\",\"IndirimOrani\",\"IlkFiyattanSatisOrani\",'Markup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    if col not in FEATS_EXCLUDED+[\"SezonGrup\"]:\n",
    "        if col in season_mean_cols:\n",
    "            train[col] = train.groupby([\"MagazaID\",\"MerchAltGrupID\",\"UrunKlasmanID\",\"Gun\"])[col].transform(\"mean\")\n",
    "        if col in season_sum_cols:\n",
    "            train[col] = train.groupby([\"MagazaID\",\"MerchAltGrupID\",\"UrunKlasmanID\",\"Gun\"])[col].transform(\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train[train.duplicated([\"Gun\",\"UrunKlasmanID\",\"MerchAltGrupID\",\"MagazaID\"])==False]\n",
    "train = train.drop([\"Gun\",\"SezonGrup\",\"Hafta\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "season_sum_cols = [\"SatisAdet\"]\n",
    "week_mean_cols = [\"OrtBirimFiyat\",\"OrtBirimMaliyet\",\"IndirimOrani\",\"KarMarji\",\"IndirimOrani\",\"IlkFiyattanSatisOrani\",\n",
    "                  'Markup','GunSonuReyonStok', 'GunSonuToplamStok','ModelSayisi','GunSonuDepoStok',\n",
    "                 'MinimumSicaklik', 'OrtalamaSicaklik', 'MaksimumSicaklik','YagisMiktari', 'KarKalinligi', 'Yagmur', 'Kar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    if col not in FEATS_EXCLUDED+[\"SezonGrup\"]:\n",
    "        if col in season_mean_cols:\n",
    "            train[col] = train.groupby([\"MagazaID\",\"MerchAltGrupID\",\"UrunKlasmanID\",\"YilHafta\"])[col].transform(\"mean\")\n",
    "        if col in season_sum_cols:\n",
    "            train[col] = train.groupby([\"MagazaID\",\"MerchAltGrupID\",\"UrunKlasmanID\",\"YilHafta\"])[col].transform(\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.drop_duplicates([\"UrunKlasmanID\",\"MerchAltGrupID\",\"MagazaID\",\"YilHafta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.to_csv(\"SatisiKesfet_TrainData_Week.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"SatisiKesfet_TrainData_Week.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "klasman = pd.read_csv(\"data/Dim_Klasman.csv\")\n",
    "train = train.merge(klasman,\"left\",[\"MerchAltGrupID\",\"UrunKlasmanID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FEATS_EXCLUDED = [\"Gun\",\"MagazaID\",\"MerchAltGrupID\",\"UrunKlasmanID\",\"SatisAdet\",\"MerchGrup\",\"MerchGrupID\",\"MerchMarkaYasGrupID\",\n",
    "                  \"BuyerGrupID\",\"KlasmanGrupID\",\"VucutBolge\",\"KlasmanIklimStatu\", \"SezonGrup\",\"YilHafta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MerchGrup Based Aggregations\n",
    "aggs = {}\n",
    "for col in train.columns:\n",
    "    if col not in FEATS_EXCLUDED:\n",
    "        aggs[col] = ['sum','mean']\n",
    "        #aggs[col] = ['sum','max','min','mean','median']\n",
    "\n",
    "aggs[\"MerchMarkaYasGrupID\"] = [\"nunique\"]        \n",
    "aggs[\"BuyerGrupID\"] = [\"nunique\"]\n",
    "aggs[\"KlasmanGrupID\"] = [\"nunique\"]\n",
    "aggs[\"VucutBolge\"] = [\"nunique\"]\n",
    "aggs[\"KlasmanIklimStatu\"] = [\"nunique\"]\n",
    "\n",
    "aggs[\"MerchAltGrupID\"] = [\"nunique\"]\n",
    "aggs[\"UrunKlasmanID\"] = [\"nunique\"]\n",
    "        \n",
    "agg1 = train.reset_index().groupby([\"MerchGrup\",\"YilHafta\"]).agg(aggs)\n",
    "agg1.columns = pd.Index([e[0] + \"_\" + e[1] for e in agg1.columns.tolist()])\n",
    "agg1.columns = ['MerchGrup_Hafta_'+ c for c in agg1.columns]\n",
    "agg1 = agg1.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MerchGrup and MerchMarkaYasGrupID Based Aggregations\n",
    "aggs = {}\n",
    "for col in train.columns:\n",
    "    if col not in FEATS_EXCLUDED:\n",
    "        aggs[col] = ['sum','mean']\n",
    "\n",
    "aggs[\"BuyerGrupID\"] = [\"nunique\"]\n",
    "aggs[\"KlasmanGrupID\"] = [\"nunique\"]\n",
    "aggs[\"VucutBolge\"] = [\"nunique\"]\n",
    "aggs[\"KlasmanIklimStatu\"] = [\"nunique\"]\n",
    "\n",
    "aggs[\"MerchAltGrupID\"] = [\"nunique\"]\n",
    "aggs[\"UrunKlasmanID\"] = [\"nunique\"]\n",
    "        \n",
    "agg2 = train.reset_index().groupby([\"MerchGrup\",\"MerchMarkaYasGrupID\",\"YilHafta\"]).agg(aggs)\n",
    "agg2.columns = pd.Index([e[0] + \"_\" + e[1] for e in agg2.columns.tolist()])\n",
    "agg2.columns = ['MerchMarkaYasGrupID_Hafta_'+ c for c in agg2.columns]\n",
    "agg2 = agg2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MerchGrup, MerchMarkaYasGrupID, MerchAltGrupID Based Aggregations\n",
    "aggs = {}\n",
    "for col in train.columns:\n",
    "    if col not in FEATS_EXCLUDED:\n",
    "        aggs[col] = ['sum','mean']\n",
    "\n",
    "aggs[\"BuyerGrupID\"] = [\"nunique\"]\n",
    "aggs[\"KlasmanGrupID\"] = [\"nunique\"]\n",
    "aggs[\"VucutBolge\"] = [\"nunique\"]\n",
    "aggs[\"KlasmanIklimStatu\"] = [\"nunique\"]\n",
    "\n",
    "aggs[\"UrunKlasmanID\"] = [\"nunique\"]\n",
    "        \n",
    "agg3 = train.reset_index().groupby([\"MerchGrup\",\"MerchMarkaYasGrupID\", \"MerchAltGrupID\",\"YilHafta\"]).agg(aggs)\n",
    "agg3.columns = pd.Index([e[0] + \"_\" + e[1] for e in agg3.columns.tolist()])\n",
    "agg3.columns = ['MerchAltGrupID_Hafta_'+ c for c in agg3.columns]\n",
    "agg3 = agg3.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MerchGrup, MerchMarkaYasGrupID, MerchAltGrupID, BuyerGrupID Based Aggregations\n",
    "aggs = {}\n",
    "for col in train.columns:\n",
    "    if col not in FEATS_EXCLUDED:\n",
    "        aggs[col] = ['sum','mean']\n",
    "\n",
    "aggs[\"KlasmanGrupID\"] = [\"nunique\"]\n",
    "aggs[\"VucutBolge\"] = [\"nunique\"]\n",
    "aggs[\"KlasmanIklimStatu\"] = [\"nunique\"]\n",
    "\n",
    "aggs[\"UrunKlasmanID\"] = [\"nunique\"]\n",
    "        \n",
    "agg4 = train.reset_index().groupby([\"MerchGrup\",\"MerchMarkaYasGrupID\", \"MerchAltGrupID\",\"BuyerGrupID\",\"YilHafta\"]).agg(aggs)\n",
    "agg4.columns = pd.Index([e[0] + \"_\" + e[1] for e in agg4.columns.tolist()])\n",
    "agg4.columns = ['BuyerGrupID_Hafta_'+ c for c in agg4.columns]\n",
    "agg4 = agg4.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MerchGrup, MerchMarkaYasGrupID, MerchAltGrupID, BuyerGrupID, KlasmanGrupID Based Aggregations\n",
    "aggs = {}\n",
    "for col in train.columns:\n",
    "    if col not in FEATS_EXCLUDED:\n",
    "        aggs[col] = ['sum','mean']\n",
    "\n",
    "aggs[\"VucutBolge\"] = [\"nunique\"]\n",
    "aggs[\"KlasmanIklimStatu\"] = [\"nunique\"]\n",
    "\n",
    "aggs[\"UrunKlasmanID\"] = [\"nunique\"]\n",
    "        \n",
    "agg5 = train.reset_index().groupby([\"MerchGrup\",\"MerchMarkaYasGrupID\", \"MerchAltGrupID\",\"BuyerGrupID\",\"KlasmanGrupID\",\"YilHafta\"]).agg(aggs)\n",
    "agg5.columns = pd.Index([e[0] + \"_\" + e[1] for e in agg5.columns.tolist()])\n",
    "agg5.columns = ['KlasmanGrupID_Hafta_'+ c for c in agg5.columns]\n",
    "agg5 = agg5.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.merge(agg1,\"left\",[\"MerchGrup\",\"YilHafta\"]).\\\n",
    "    merge(agg2,\"left\",[\"MerchGrup\",\"MerchMarkaYasGrupID\",\"YilHafta\"]).\\\n",
    "    merge(agg3,\"left\",[\"MerchGrup\",\"MerchMarkaYasGrupID\",\"MerchAltGrupID\",\"YilHafta\"]).\\\n",
    "    merge(agg4,\"left\",[\"MerchGrup\",\"MerchMarkaYasGrupID\",\"MerchAltGrupID\",\"BuyerGrupID\",\"YilHafta\"]).\\\n",
    "    merge(agg5,\"left\",[\"MerchGrup\",\"MerchMarkaYasGrupID\",\"MerchAltGrupID\",\"BuyerGrupID\",\"KlasmanGrupID\",\"YilHafta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train.to_csv(\"SatisiKesfet_TrainData_Week_Final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10817221, 257)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These columns were dropped: ['MerchGrup_Hafta_KlasmanIklimStatu_nunique', 'MerchGrup_Hafta_VucutBolge_nunique', 'MerchMarkaYasGrupID_Hafta_KlasmanIklimStatu_nunique', 'KlasmanGrupID_Hafta_KlasmanIklimStatu_nunique'] \n",
      " Since they have some values covering 99.9% of its all values\n"
     ]
    }
   ],
   "source": [
    "# Dropping columns with highly frequent values\n",
    "cols_to_drop = []\n",
    "drop_values = []\n",
    "for col in train.columns:\n",
    "    temp = train[col].value_counts(dropna=False, normalize=1)\n",
    "    if len(temp[temp>0.99].index)>0:\n",
    "        cols_to_drop.append(col)\n",
    "        drop_values.append(temp)  \n",
    "#train.drop(cols_to_drop, axis = 1, inplace = True)\n",
    "print(\"These columns were dropped: {} \\n Since they have some values covering 99.9% of its all values\".format(cols_to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.drop(cols_to_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FEATS_EXCLUDED = [\"Gun\",\"MagazaID\",\"MerchAltGrupID\",\"UrunKlasmanID\",\"SatisAdet\",\"MerchGrup\",\"MerchGrupID\",\"MerchMarkaYasGrupID\",\n",
    "                  \"BuyerGrupID\",\"KlasmanGrupID\",\"VucutBolge\",\"KlasmanIklimStatu\",\"YilHafta\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# params optimized by optuna\n",
    "params = {'task':'train','objective': 'regression','metric': 'rmse','learning_rate': 0.05,'verbose': -1,'nthread':-1,\n",
    "          'num_leaves': 10, 'min_data': 50, 'max_depth': 7, 'min_data_in_leaf': 50, 'feature_fraction': 0.8,'bagging_fraction': 0.8,'bagging_freq': 2 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kfold_lightgbm(params, train_df, num_folds,FEATS_EXCLUDED):\n",
    "    print(\"Starting LightGBM. Train shape: {}\".format(train_df.shape))\n",
    "    folds = KFold(n_splits= num_folds, shuffle=True, random_state=326)\n",
    "\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    final_rmse = 0\n",
    "    final_mape = 0\n",
    "    final_mae = 0\n",
    "    feats = [f for f in train_df.columns if f not in FEATS_EXCLUDED]\n",
    "    clfs = []\n",
    "    # k-fold\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['SatisAdet'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['SatisAdet'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['SatisAdet'].iloc[valid_idx]\n",
    "\n",
    "        # set data structure\n",
    "        lgb_train = lgb.Dataset(train_x,label=train_y,free_raw_data=False)\n",
    "        lgb_test = lgb.Dataset(valid_x,label=valid_y,free_raw_data=False)\n",
    "\n",
    "        reg = lgb.train(params,lgb_train,valid_sets=[lgb_train, lgb_test],valid_names=['train', 'valid'],\n",
    "                        num_boost_round=100000,early_stopping_rounds= 100,verbose_eval=200)\n",
    "        clfs.append(reg)\n",
    "        oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)\n",
    "        #sub_preds += reg.predict(test_df[feats], num_iteration=reg.best_iteration) / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = np.log1p(reg.feature_importance(importance_type='gain', iteration=reg.best_iteration))\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        curr_rmse = sqrt(mean_squared_error(valid_y, oof_preds[valid_idx]))\n",
    "        curr_mape = mape(valid_y, oof_preds[valid_idx]) \n",
    "        curr_mae = mean_absolute_error(valid_y, oof_preds[valid_idx])\n",
    "        \n",
    "        #curr_rmse = roc_auc_score(valid_y, oof_preds[valid_idx])\n",
    "        final_rmse += curr_rmse/num_folds\n",
    "        final_mape += curr_mape/num_folds\n",
    "        final_mae += curr_mae/num_folds\n",
    "        print('Fold %2d rmse : %.6f' % (n_fold + 1, curr_rmse))\n",
    "        print('Fold %2d mae : %.6f' % (n_fold + 1, curr_mae))\n",
    "\n",
    "        del reg, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "              \n",
    "    print('Averall RMSE : %.6f' % (final_rmse))\n",
    "    print('Averall MAE : %.6f' % (final_mae))\n",
    "\n",
    "    # save submission file\n",
    "    #test_df.loc[:,'SatisAdet'] = sub_preds\n",
    "    #test_df = test_df.reset_index()\n",
    "    #test_df = test_df[['card_id', 'SatisAdet']]\n",
    "    #submission_file_name = \"lgbm_\"+str(np.round(final_rmse,4))+\".csv\"\n",
    "    #test_df.to_csv(DATAPATH+submission_file_name, index=False)    \n",
    "        \n",
    "    return clfs, oof_preds, feature_importance_df, final_rmse    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LightGBM. Train shape: (10817221, 253)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttrain's rmse: 6.06294\tvalid's rmse: 6.1268\n",
      "[400]\ttrain's rmse: 5.92368\tvalid's rmse: 6.01224\n",
      "[600]\ttrain's rmse: 5.83773\tvalid's rmse: 5.95535\n",
      "[800]\ttrain's rmse: 5.77433\tvalid's rmse: 5.91347\n",
      "[1000]\ttrain's rmse: 5.72504\tvalid's rmse: 5.88611\n",
      "[1200]\ttrain's rmse: 5.6823\tvalid's rmse: 5.86097\n",
      "[1400]\ttrain's rmse: 5.64424\tvalid's rmse: 5.84105\n",
      "[1600]\ttrain's rmse: 5.61152\tvalid's rmse: 5.82434\n",
      "[1800]\ttrain's rmse: 5.58052\tvalid's rmse: 5.81172\n",
      "[2000]\ttrain's rmse: 5.55385\tvalid's rmse: 5.80363\n",
      "[2200]\ttrain's rmse: 5.52851\tvalid's rmse: 5.79414\n",
      "[2400]\ttrain's rmse: 5.50286\tvalid's rmse: 5.78383\n",
      "[2600]\ttrain's rmse: 5.48112\tvalid's rmse: 5.77636\n",
      "[2800]\ttrain's rmse: 5.4606\tvalid's rmse: 5.76968\n",
      "[3000]\ttrain's rmse: 5.44145\tvalid's rmse: 5.76475\n",
      "[3200]\ttrain's rmse: 5.42386\tvalid's rmse: 5.76029\n",
      "[3400]\ttrain's rmse: 5.40459\tvalid's rmse: 5.75377\n",
      "[3600]\ttrain's rmse: 5.38594\tvalid's rmse: 5.74743\n",
      "[3800]\ttrain's rmse: 5.36825\tvalid's rmse: 5.74126\n",
      "[4000]\ttrain's rmse: 5.35291\tvalid's rmse: 5.73778\n",
      "[4200]\ttrain's rmse: 5.33785\tvalid's rmse: 5.73457\n",
      "[4400]\ttrain's rmse: 5.32201\tvalid's rmse: 5.73012\n",
      "[4600]\ttrain's rmse: 5.3075\tvalid's rmse: 5.7252\n",
      "[4800]\ttrain's rmse: 5.29247\tvalid's rmse: 5.72196\n",
      "[5000]\ttrain's rmse: 5.27622\tvalid's rmse: 5.71569\n",
      "[5200]\ttrain's rmse: 5.26177\tvalid's rmse: 5.71095\n",
      "[5400]\ttrain's rmse: 5.24963\tvalid's rmse: 5.70855\n",
      "[5600]\ttrain's rmse: 5.23596\tvalid's rmse: 5.70461\n",
      "[5800]\ttrain's rmse: 5.22186\tvalid's rmse: 5.70052\n",
      "[6000]\ttrain's rmse: 5.20884\tvalid's rmse: 5.69613\n",
      "[6200]\ttrain's rmse: 5.1965\tvalid's rmse: 5.69318\n",
      "[6400]\ttrain's rmse: 5.18613\tvalid's rmse: 5.69222\n",
      "[6600]\ttrain's rmse: 5.1739\tvalid's rmse: 5.68866\n",
      "[6800]\ttrain's rmse: 5.16292\tvalid's rmse: 5.68708\n",
      "[7000]\ttrain's rmse: 5.15209\tvalid's rmse: 5.68418\n",
      "[7200]\ttrain's rmse: 5.14179\tvalid's rmse: 5.68222\n",
      "[7400]\ttrain's rmse: 5.13136\tvalid's rmse: 5.68049\n",
      "[7600]\ttrain's rmse: 5.12083\tvalid's rmse: 5.67822\n",
      "[7800]\ttrain's rmse: 5.11042\tvalid's rmse: 5.67649\n",
      "Early stopping, best iteration is:\n",
      "[7716]\ttrain's rmse: 5.11383\tvalid's rmse: 5.67615\n",
      "Fold  1 rmse : 5.676147\n",
      "Fold  1 mae : 2.001101\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttrain's rmse: 6.06336\tvalid's rmse: 6.11056\n",
      "[400]\ttrain's rmse: 5.92744\tvalid's rmse: 6.00032\n",
      "[600]\ttrain's rmse: 5.84081\tvalid's rmse: 5.94357\n",
      "[800]\ttrain's rmse: 5.77838\tvalid's rmse: 5.90519\n",
      "[1000]\ttrain's rmse: 5.72826\tvalid's rmse: 5.87562\n",
      "[1200]\ttrain's rmse: 5.68517\tvalid's rmse: 5.85387\n",
      "[1400]\ttrain's rmse: 5.64698\tvalid's rmse: 5.8395\n",
      "[1600]\ttrain's rmse: 5.61203\tvalid's rmse: 5.82154\n",
      "[1800]\ttrain's rmse: 5.57771\tvalid's rmse: 5.80657\n",
      "[2000]\ttrain's rmse: 5.54837\tvalid's rmse: 5.7961\n",
      "[2200]\ttrain's rmse: 5.52304\tvalid's rmse: 5.78792\n",
      "[2400]\ttrain's rmse: 5.49901\tvalid's rmse: 5.78016\n",
      "[2600]\ttrain's rmse: 5.47731\tvalid's rmse: 5.77446\n",
      "[2800]\ttrain's rmse: 5.45597\tvalid's rmse: 5.76806\n",
      "[3000]\ttrain's rmse: 5.43651\tvalid's rmse: 5.76438\n",
      "[3200]\ttrain's rmse: 5.41829\tvalid's rmse: 5.76005\n",
      "[3400]\ttrain's rmse: 5.39996\tvalid's rmse: 5.75497\n",
      "[3600]\ttrain's rmse: 5.38252\tvalid's rmse: 5.74961\n",
      "[3800]\ttrain's rmse: 5.36553\tvalid's rmse: 5.74413\n",
      "[4000]\ttrain's rmse: 5.34881\tvalid's rmse: 5.73914\n",
      "[4200]\ttrain's rmse: 5.33251\tvalid's rmse: 5.73533\n",
      "[4400]\ttrain's rmse: 5.31735\tvalid's rmse: 5.73206\n",
      "[4600]\ttrain's rmse: 5.30303\tvalid's rmse: 5.72946\n",
      "[4800]\ttrain's rmse: 5.28984\tvalid's rmse: 5.72769\n",
      "[5000]\ttrain's rmse: 5.27568\tvalid's rmse: 5.72325\n",
      "[5200]\ttrain's rmse: 5.26268\tvalid's rmse: 5.72048\n",
      "[5400]\ttrain's rmse: 5.24937\tvalid's rmse: 5.71886\n",
      "[5600]\ttrain's rmse: 5.23656\tvalid's rmse: 5.71744\n",
      "[5800]\ttrain's rmse: 5.22412\tvalid's rmse: 5.71474\n",
      "[6000]\ttrain's rmse: 5.21148\tvalid's rmse: 5.71103\n",
      "[6200]\ttrain's rmse: 5.19872\tvalid's rmse: 5.70679\n",
      "[6400]\ttrain's rmse: 5.18669\tvalid's rmse: 5.7042\n",
      "[6600]\ttrain's rmse: 5.17528\tvalid's rmse: 5.70209\n",
      "[6800]\ttrain's rmse: 5.16421\tvalid's rmse: 5.69987\n",
      "[7000]\ttrain's rmse: 5.1533\tvalid's rmse: 5.69802\n",
      "[7200]\ttrain's rmse: 5.14208\tvalid's rmse: 5.69565\n",
      "[7400]\ttrain's rmse: 5.13077\tvalid's rmse: 5.69354\n",
      "[7600]\ttrain's rmse: 5.12106\tvalid's rmse: 5.69291\n",
      "[7800]\ttrain's rmse: 5.11077\tvalid's rmse: 5.69123\n",
      "[8000]\ttrain's rmse: 5.10078\tvalid's rmse: 5.68952\n",
      "[8200]\ttrain's rmse: 5.09138\tvalid's rmse: 5.68752\n",
      "[8400]\ttrain's rmse: 5.08162\tvalid's rmse: 5.68576\n",
      "Early stopping, best iteration is:\n",
      "[8474]\ttrain's rmse: 5.07837\tvalid's rmse: 5.68516\n",
      "Fold  2 rmse : 5.685156\n",
      "Fold  2 mae : 2.003503\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttrain's rmse: 6.07825\tvalid's rmse: 6.07416\n",
      "[400]\ttrain's rmse: 5.93965\tvalid's rmse: 5.96447\n",
      "[600]\ttrain's rmse: 5.8536\tvalid's rmse: 5.90651\n",
      "[800]\ttrain's rmse: 5.7896\tvalid's rmse: 5.87023\n",
      "[1000]\ttrain's rmse: 5.73858\tvalid's rmse: 5.83951\n",
      "[1200]\ttrain's rmse: 5.69661\tvalid's rmse: 5.82033\n",
      "[1400]\ttrain's rmse: 5.65746\tvalid's rmse: 5.80068\n",
      "[1600]\ttrain's rmse: 5.62331\tvalid's rmse: 5.78541\n",
      "[1800]\ttrain's rmse: 5.59039\tvalid's rmse: 5.76917\n",
      "[2000]\ttrain's rmse: 5.56246\tvalid's rmse: 5.76032\n",
      "[2200]\ttrain's rmse: 5.53746\tvalid's rmse: 5.75172\n",
      "[2400]\ttrain's rmse: 5.51257\tvalid's rmse: 5.7424\n",
      "[2600]\ttrain's rmse: 5.48857\tvalid's rmse: 5.73446\n",
      "[2800]\ttrain's rmse: 5.46817\tvalid's rmse: 5.72932\n",
      "[3000]\ttrain's rmse: 5.44789\tvalid's rmse: 5.72207\n",
      "[3200]\ttrain's rmse: 5.4284\tvalid's rmse: 5.71618\n",
      "[3400]\ttrain's rmse: 5.4108\tvalid's rmse: 5.71087\n",
      "[3600]\ttrain's rmse: 5.39363\tvalid's rmse: 5.70762\n",
      "[3800]\ttrain's rmse: 5.37571\tvalid's rmse: 5.70135\n",
      "[4000]\ttrain's rmse: 5.35924\tvalid's rmse: 5.69624\n",
      "[4200]\ttrain's rmse: 5.34357\tvalid's rmse: 5.69246\n",
      "[4400]\ttrain's rmse: 5.32792\tvalid's rmse: 5.68863\n",
      "[4600]\ttrain's rmse: 5.31329\tvalid's rmse: 5.68635\n",
      "[4800]\ttrain's rmse: 5.29937\tvalid's rmse: 5.68346\n",
      "[5000]\ttrain's rmse: 5.28484\tvalid's rmse: 5.67889\n",
      "[5200]\ttrain's rmse: 5.27104\tvalid's rmse: 5.67594\n",
      "[5400]\ttrain's rmse: 5.25805\tvalid's rmse: 5.67314\n",
      "[5600]\ttrain's rmse: 5.24471\tvalid's rmse: 5.66916\n",
      "[5800]\ttrain's rmse: 5.23171\tvalid's rmse: 5.66634\n",
      "[6000]\ttrain's rmse: 5.2197\tvalid's rmse: 5.6636\n",
      "[6200]\ttrain's rmse: 5.20809\tvalid's rmse: 5.66176\n",
      "[6400]\ttrain's rmse: 5.19638\tvalid's rmse: 5.65953\n",
      "[6600]\ttrain's rmse: 5.1849\tvalid's rmse: 5.65637\n",
      "[6800]\ttrain's rmse: 5.17415\tvalid's rmse: 5.6541\n",
      "[7000]\ttrain's rmse: 5.1631\tvalid's rmse: 5.65273\n",
      "[7200]\ttrain's rmse: 5.1527\tvalid's rmse: 5.64958\n",
      "[7400]\ttrain's rmse: 5.14177\tvalid's rmse: 5.64693\n",
      "[7600]\ttrain's rmse: 5.13053\tvalid's rmse: 5.6439\n",
      "[7800]\ttrain's rmse: 5.12075\tvalid's rmse: 5.64256\n",
      "[8000]\ttrain's rmse: 5.11031\tvalid's rmse: 5.63907\n",
      "[8200]\ttrain's rmse: 5.1008\tvalid's rmse: 5.6377\n",
      "[8400]\ttrain's rmse: 5.0916\tvalid's rmse: 5.63692\n",
      "[8600]\ttrain's rmse: 5.08269\tvalid's rmse: 5.63614\n",
      "Early stopping, best iteration is:\n",
      "[8546]\ttrain's rmse: 5.08502\tvalid's rmse: 5.63568\n",
      "Fold  3 rmse : 5.635682\n",
      "Fold  3 mae : 2.002431\n",
      "Averall RMSE : 5.665662\n",
      "Averall MAE : 2.002345\n"
     ]
    }
   ],
   "source": [
    "clfs, oof_preds, feature_importance_df, final_rmse = kfold_lightgbm(params,train, num_folds = 3, FEATS_EXCLUDED=FEATS_EXCLUDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('weekly_lgbm.pkl', 'wb') as f:\n",
    "    pickle.dump(clfs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_df = train[[\"YilHafta\",\"MagazaID\",\"MerchAltGrupID\",\"UrunKlasmanID\",\"SatisAdet\"]].copy()\n",
    "pred_df[\"pred\"] = oof_preds\n",
    "pred_df.to_csv(\"weekly_oof.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_df.loc[pred_df.SatisAdet==0, \"pred\"] = 0\n",
    "pred_df.to_csv(\"weekly_oof_post.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-75c7acc806a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SatisiKesfet_TestData_Week_Final.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFEATS_EXCLUDED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msub_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msub_preds\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"SatisiKesfet_TestData_Week_Final.csv\")\n",
    "feats = [f for f in train.columns if f not in FEATS_EXCLUDED]\n",
    "sub_preds = np.zeros(train.shape[0])\n",
    "for clf in clfs:\n",
    "    sub_preds += clf.predict(test[feats], num_iteration=reg.best_iteration) / len(clfs)\n",
    "sub_df = test[[\"YilHafta\",\"MagazaID\",\"MerchAltGrupID\",\"UrunKlasmanID\"]].copy()\n",
    "sub_df[\"pred\"] = sub_preds\n",
    "sub_df.to_csv(\"weekly_preds.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_preds = np.zeros(test.shape[0])\n",
    "for clf in clfs:\n",
    "    sub_preds += clf.predict(test[feats], num_iteration=clf.best_iteration) / len(clfs)\n",
    "sub_df = test[[\"YilHafta\",\"MagazaID\",\"MerchAltGrupID\",\"UrunKlasmanID\"]].copy()\n",
    "sub_df[\"pred\"] = sub_preds\n",
    "sub_df.to_csv(\"weekly_preds.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importance_df.to_csv(\"feature_importance_weekly.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:         380166      282983       87953           0        9228       95150\r\n",
      "Swap:             0           0           0\r\n"
     ]
    }
   ],
   "source": [
    "!free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sub_df.loc[sub_df.SatisAdet==0, \"pred\"] = 0\n",
    "#sub_df.to_csv(\"weekly_oof_post.csv\", index = False)1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrtBirimFiyat</th>\n",
       "      <th>SatisAdet</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15562</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15666</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16097</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46323</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53235</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       OrtBirimFiyat  SatisAdet  pred\n",
       "15562            0.0          0   0.0\n",
       "15666            0.0          0   0.0\n",
       "16097            0.0          0   0.0\n",
       "46323            0.0          0   0.0\n",
       "53235            0.0          0   0.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_valid[mape_valid.OrtBirimFiyat==0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averall RMSE : 5.399932\n",
      "Averall MAPE : 80.196364\n",
      "Averall MAE : 1.617424\n"
     ]
    }
   ],
   "source": [
    "mape_valid = train[[\"OrtBirimFiyat\",\"SatisAdet\"]].copy()\n",
    "mape_valid[\"pred\"] = oof_preds\n",
    "mape_valid.loc[mape_valid[\"OrtBirimFiyat\"]==0, \"pred\"] = 0\n",
    "mape_valid.loc[mape_valid[\"SatisAdet\"]==0, \"pred\"] = 0\n",
    "mape_valid2 = mape_valid[mape_valid.SatisAdet!=0]\n",
    "curr_rmse = sqrt(mean_squared_error(mape_valid.SatisAdet, mape_valid.pred))\n",
    "curr_mape = mape(mape_valid2.SatisAdet, mape_valid2.pred)\n",
    "curr_mae = mean_absolute_error(mape_valid.SatisAdet, mape_valid.pred)\n",
    "print('Averall RMSE : %.6f' % (curr_rmse))\n",
    "print('Averall MAPE : %.6f' % (curr_mape))\n",
    "print('Averall MAE : %.6f' % (curr_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "def kfold_ridge(train_df, num_folds,FEATS_EXCLUDED):\n",
    "    print(\"Starting LightGBM. Train shape: {}\".format(train_df.shape))\n",
    "    folds = KFold(n_splits= num_folds, shuffle=True, random_state=326)\n",
    "\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    final_rmse = 0\n",
    "    final_mae = 0\n",
    "    feats = [f for f in train_df.columns if f not in FEATS_EXCLUDED]\n",
    "    clfs = []\n",
    "    # k-fold\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['SatisAdet'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['SatisAdet'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['SatisAdet'].iloc[valid_idx]\n",
    "\n",
    "        reg = Lasso(alpha=0) # higher the alpha value, more restriction on the coefficients; low alpha > more generalization, coefficients are barely\n",
    "        reg.fit(train_x, train_y)\n",
    "        clfs.append(reg)\n",
    "        oof_preds[valid_idx] = reg.predict(valid_x)\n",
    "        #sub_preds += reg.predict(test_df[feats], num_iteration=reg.best_iteration) / folds.n_splits\n",
    "        curr_rmse = sqrt(mean_squared_error(valid_y, oof_preds[valid_idx]))\n",
    "        curr_mae = mean_absolute_error(valid_y, oof_preds[valid_idx])\n",
    "        \n",
    "        #curr_rmse = roc_auc_score(valid_y, oof_preds[valid_idx])\n",
    "        final_rmse += curr_rmse/num_folds\n",
    "        final_mae += curr_mae/num_folds\n",
    "        print('Fold %2d rmse : %.6f' % (n_fold + 1, curr_rmse))\n",
    "        print('Fold %2d mae : %.6f' % (n_fold + 1, curr_mae))\n",
    "\n",
    "        del reg, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "              \n",
    "    print('Averall RMSE : %.6f' % (final_rmse))\n",
    "    print('Averall MAE : %.6f' % (final_mae))\n",
    "    \n",
    "    return clfs, oof_preds, final_rmse    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clfs, oof_preds, final_rmse = kfold_ridge(train, num_folds=5, FEATS_EXCLUDED=FEATS_EXCLUDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('weekly_ridge.pkl', 'wb') as f:\n",
    "    pickle.dump(clfs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso & Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "feats = [f for f in train.columns if f not in FEATS_EXCLUDED]\n",
    "rr = Ridge(alpha=0.01) # higher the alpha value, more restriction on the coefficients; low alpha > more generalization, coefficients are barely\n",
    "rr.fit(train.loc[train.Yil<2017, feats], train.loc[train.Yil<2017, \"SatisAdet\"])\n",
    "preds = rr.predict(train.loc[train.Yil==2017, feats])\n",
    "curr_rmse = sqrt(mean_squared_error(train.loc[train.Yil==2017,\"SatisAdet\"], preds))\n",
    "curr_mae = mean_absolute_error(train.loc[train.Yil==2017,\"SatisAdet\"], preds)\n",
    "print('Averall RMSE : %.6f' % (curr_rmse))\n",
    "print('Averall MAE : %.6f' % (curr_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mape_valid = train.loc[train.Yil==2017, [\"OrtBirimFiyat\",\"SatisAdet\"]].copy()\n",
    "mape_valid[\"pred\"] = preds\n",
    "mape_valid.loc[mape_valid[\"OrtBirimFiyat\"]==0, \"pred\"] = 0\n",
    "mape_valid2 = mape_valid[mape_valid.SatisAdet!=0]\n",
    "curr_rmse = sqrt(mean_squared_error(mape_valid.SatisAdet, mape_valid.pred))\n",
    "curr_mape = mape(mape_valid2.SatisAdet, mape_valid2.pred)\n",
    "curr_mae = mean_absolute_error(mape_valid.SatisAdet, mape_valid.pred)\n",
    "print('Averall RMSE : %.6f' % (curr_rmse))\n",
    "print('Averall MAPE : %.6f' % (curr_mape))\n",
    "print('Averall MAE : %.6f' % (curr_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "feats = [f for f in train.columns if f not in FEATS_EXCLUDED]\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "rf.fit(train.loc[train.Yil<2017, feats], train.loc[train.Yil<2017, \"SatisAdet\"])\n",
    "preds = rf.predict(train.loc[train.Yil==2017, feats])\n",
    "curr_rmse = sqrt(mean_squared_error(train.loc[train.Yil==2017,\"SatisAdet\"], preds))\n",
    "curr_mae = mean_absolute_error(train.loc[train.Yil==2017,\"SatisAdet\"], preds)\n",
    "print('Averall RMSE : %.6f' % (curr_rmse))\n",
    "print('Averall MAE : %.6f' % (curr_mae))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
